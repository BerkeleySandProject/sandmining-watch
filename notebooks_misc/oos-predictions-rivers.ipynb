{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf92aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6396a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import glob\n",
    "\n",
    "from rasterio.plot import show\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a6dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from project_config import GCP_PROJECT_NAME, DATASET_JSON_PATH\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import gcp_utils\n",
    "\n",
    "# gcp_cred_file = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "# with open(gcp_cred_file) as source:\n",
    "#     info = json.load(source)\n",
    "\n",
    "# storage_credentials = service_account.Credentials.from_service_account_info(info)\n",
    "\n",
    "gcp_client = storage.Client(project=GCP_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42050e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suraj.nair/.conda/envs/rio-cog-env\n"
     ]
    }
   ],
   "source": [
    "#### Configuration\n",
    "\n",
    "from experiment_configs.configs import unet_config, satmae_large_config, satmae_large_inf_config\n",
    "# config = satmae_large_config\n",
    "config = satmae_large_inf_config\n",
    "\n",
    "from utils.rastervision_pipeline import create_s2_image_source, create_scene_s2, scene_to_inference_ds\n",
    "from ml.learner import BinarySegmentationPredictor\n",
    "from models.model_factory import model_factory\n",
    "\n",
    "import subprocess\n",
    "ENVBIN = f\"{os.environ['HOME']}/.conda/envs/rio-cog-env\"\n",
    "print(ENVBIN)\n",
    "\n",
    "\n",
    "from project_config import CLASS_CONFIG\n",
    "from ml.eval_utils import save_predictions\n",
    "\n",
    "\n",
    "def create_cogs(source_file, out_file):\n",
    "    cog_create = f\"rio cogeo create {source_file} {out_file}\"\n",
    "    try:\n",
    "        subprocess.run(cog_create,\n",
    "                   cwd = f\"{ENVBIN}/bin\",\n",
    "                   shell=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(e)\n",
    "        \n",
    "def upload_blob(storage_client, bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd148c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(storage_client, bucket_name, out_file_name, data_for_json):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \n",
    "    json_data = json.dumps(data_for_json, indent = 2)\n",
    "    blob = bucket.blob(out_file_name)\n",
    "    blob.upload_from_string(data= json_data ,\n",
    "                            content_type='application/json')\n",
    "    \n",
    "def read_json(storage_client, bucket_name, file_name):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = BUCKET.get_blob(file_name)\n",
    "    # load blob using json\n",
    "    data = json.loads(blob.download_as_string())\n",
    "    return data\n",
    "\n",
    "\n",
    "def delete_files(filepath):\n",
    "    try:\n",
    "        cmd = f\"rm -f {filepath}/*\"\n",
    "        # Run the gsutil command\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "        print(f\"Existing file deleted\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def save_predictions_to_gcp(prediction, \n",
    "                     prediction_out_path, \n",
    "                    crs_transformer, \n",
    "                    prediction_scores_file_path, \n",
    "                    prediction_cog_file_path, \n",
    "                    gcp_dest,\n",
    "                    overwrite = False,\n",
    "                    move_to_gcp = True):\n",
    "    \n",
    "#     print(prediction_out_path, prediction_scores_file_path, prediction_cog_file_path, sep = \"\\n\")\n",
    "    ### SAVE\n",
    "    \n",
    "    print(f\"Saving to {prediction_out_path}\", datetime.now())\n",
    "    if not os.path.exists(prediction_out_path):\n",
    "        os.makedirs(prediction_out_path)\n",
    "    else:\n",
    "        if overwrite:\n",
    "            print(\"Prediction Files Exist! Overwriting..\")\n",
    "            delete_files(prediction_out_path)\n",
    "            \n",
    "            save_predictions(prediction, \n",
    "                 path=prediction_out_path, \n",
    "                 class_config=CLASS_CONFIG, \n",
    "                 crs_transformer=crs_transformer, \n",
    "                 threshold=0.5)\n",
    "                \n",
    "            create_cogs(prediction_scores_file_path, prediction_cog_file_path)\n",
    "    \n",
    "            if move_to_gcp:\n",
    "                ### Move to GCP\n",
    "                gcp_move = f\"\"\"gsutil cp -r {prediction_cog_file_path} {gcp_dest}\"\"\"\n",
    "                try:\n",
    "                    # Run the gsutil command\n",
    "                    subprocess.run(gcp_move, shell=True, check=True)\n",
    "                    print(f\"Files copied successfully to GCP Bucket\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error copying files: {e}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Prediction Files Exist! Set overwrite = True to over write!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    print(\"Completed\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f186c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = \"1Ov1M_zsb5jYo_dtIjUXco1wvNgasmdoZKtJ877psHL4\"\n",
    "sheet_name = \"rivers_to_osm_label\"\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        \n",
    "df_rivers = pd.read_csv(url)\n",
    "df_rivers['osm_id'] = df_rivers['osm_id'].astype('int')\n",
    "\n",
    "river_names = df_rivers['River'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d78ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'sand_mining_inference'\n",
    "OVERWRITE = True\n",
    "N_CHANNELS = 10\n",
    "\n",
    "drop = ['Godavari', 'Godavari (N)',\n",
    "       'Godavari (S)', 'Sone - South']\n",
    "river_names = [r.lower() for r in river_names if r not in drop]\n",
    "\n",
    "\n",
    "DATE = '2022-10-01'\n",
    "date_filter = lambda x: x['date'] == DATE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "32733746",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load River URI dataset\n",
    "master_file = \"../dataset/river_oos_dataset_v0.1.json\"\n",
    "\n",
    "river_jsons = json.load(open(master_file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "820287d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SatMae: Loading encoder weights from /data/sand_mining/checkpoints/finetuned/SatMAE-L_LoRA-bias_LN_160px_mclw-6_B8_E9_SmoothVal-S5-DecOnly-E20.pth\n",
      "Number of parameters loaded: 299\n",
      "SatMae: Loading decoder weights from /data/sand_mining/checkpoints/finetuned/SatMAE-L_LoRA-bias_LN_160px_mclw-6_B8_E9_SmoothVal-S5-DecOnly-E20.pth\n",
      "Temperature scaling set to None\n",
      "trainable params: 304.273667M || all params: 304.273667M || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from ml.model_stats import count_number_of_weights\n",
    "model = model_factory(\n",
    "    config,\n",
    "    n_channels= N_CHANNELS ,\n",
    ")\n",
    "\n",
    "predictor = BinarySegmentationPredictor(\n",
    "    config, model, config.encoder_weights_path\n",
    ")\n",
    "\n",
    "crop_sz = int(config.tile_size // 5) #20% of the tiles at the edges are discarded\n",
    "\n",
    "all_params, trainable_params = count_number_of_weights(predictor.model)\n",
    "print(f\"trainable params: {trainable_params/1e6}M || all params: {all_params/1e6}M || trainable%: {100 * trainable_params / all_params:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b4d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 14:11:19:rastervision.pipeline.file_system.utils: INFO - Downloading https://storage.googleapis.com/sand_mining_inference/mahanadi/2022-10-01/S2/mahanadi_s2_2022-10-01.tif to /tmp/tmpw6mirneu/cache/http/storage.googleapis.com/sand_mining_inference/mahanadi/2022-10-01/S2/mahanadi_s2_2022-10-01.tif...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahanadi 2022-10-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cbe468fa4c4d84a5eb7ef22661b462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 11%|#1        | 248M/2.12G [00:05<00:38, 52.1MB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 14:11:56:rastervision.pipeline.file_system.utils: INFO - Using cached file /tmp/tmpw6mirneu/cache/http/storage.googleapis.com/sand_mining_inference/mahanadi/mahanadi.geojson.\n"
     ]
    }
   ],
   "source": [
    "for river_name in ['mahanadi']:\n",
    "    gc.collect()\n",
    "    if river_name not in drop:\n",
    "        print(river_name, DATE)\n",
    "        \n",
    "        BASE_PATH = f\"/data/sand_mining/predictions/outputs/{river_name}\"\n",
    "        RIVER_URI = f\"https://storage.googleapis.com/sand_mining_inference/{river_name}/{river_name}.geojson\"\n",
    "        RIVER_OUT_NAME = river_name\n",
    "        filter_func = lambda x: (x['date'] == DATE) & (x['river'] == river_name)\n",
    "        \n",
    "        river_json = list(filter(filter_func, river_jsons))[0]\n",
    "        uri_to_s2 = river_json['uri_to_s2']\n",
    "        \n",
    "        prediction_out_path = os.path.join(BASE_PATH, DATE)\n",
    "        prediction_scores_file_path = os.path.join(prediction_out_path,  \"scores.tif\")\n",
    "        prediction_cog_file_path = os.path.join(prediction_out_path, f\"{RIVER_OUT_NAME}_prediction_{DATE}.tif\")\n",
    "        gcp_dest = f\"gs://sand_mining_inference/{river_name}/{DATE}/{RIVER_OUT_NAME}_prediction_{DATE}.tif\"\n",
    "\n",
    "    #     #Predict\n",
    "        r_source = create_scene_s2(config, uri_to_s2, label_uri = None, scene_id = 0, rivers_uri = RIVER_URI)\n",
    "        r_inference = scene_to_inference_ds(config, r_source, full_image=False, stride=int(config.tile_size/2))\n",
    "        crs_transformer = r_inference.scene.raster_source.crs_transformer\n",
    "        prediction = predictor.predict_site(r_inference, crop_sz=crop_sz)\n",
    "\n",
    "        save_predictions_to_gcp(\n",
    "                prediction, \n",
    "                prediction_out_path, \n",
    "                crs_transformer, \n",
    "                prediction_scores_file_path, \n",
    "                prediction_cog_file_path, \n",
    "                gcp_dest,\n",
    "                overwrite = OVERWRITE,\n",
    "                move_to_gcp = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rv-21",
   "language": "python",
   "name": "rv-21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
