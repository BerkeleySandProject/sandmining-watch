{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ede6a-cec2-4306-b462-430933a476f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from os.path import expanduser\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from google.cloud import storage\n",
    "from project_config import GCP_PROJECT_NAME\n",
    "\n",
    "gcp_client = storage.Client(project=GCP_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184df66e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76aeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_configs.configs import *\n",
    "config = satmae_large_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b9bc0",
   "metadata": {},
   "source": [
    "### Create Rastervision datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda67a90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from utils.rastervision_pipeline import observation_to_scene, scene_to_training_ds, scene_to_validation_ds, warn_if_nan_in_raw_raster\n",
    "from utils.data_management import observation_factory\n",
    "\n",
    "from project_config import is_training, is_validation\n",
    "\n",
    "all_observations = observation_factory(gcp_client)\n",
    "training_scenes = []\n",
    "validation_scenes = []\n",
    "\n",
    "for observation in all_observations:\n",
    "    is_train = is_training(observation.name)\n",
    "    is_val = is_validation(observation.name)\n",
    "\n",
    "    assert not (is_train and is_val), \"An observation cannot be in both training and validation\"\n",
    "\n",
    "    scene = observation_to_scene(config, observation)    \n",
    "    if is_train:\n",
    "        training_scenes.append(scene)\n",
    "    elif is_val:\n",
    "        validation_scenes.append(scene)\n",
    "    else:\n",
    "        print(f\"Ignoring observation {observation.name}\")\n",
    "\n",
    "\n",
    "#all_scenes = training_scenes + validation_scenes\n",
    "#for scene in all_scenes:\n",
    "#    warn_if_nan_in_raw_raster(scene.raster_source)\n",
    "\n",
    "training_datasets = [\n",
    "    scene_to_training_ds(config, scene) for scene in training_scenes #random window sampling happens here\n",
    "]\n",
    "validation_datasets = [\n",
    "    scene_to_validation_ds(config, scene) for scene in validation_scenes\n",
    "]\n",
    "\n",
    "train_dataset_merged = ConcatDataset(training_datasets)\n",
    "val_dataset_merged = ConcatDataset(validation_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd6fec",
   "metadata": {},
   "source": [
    "## Visualize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizing import visualize_dataset\n",
    "\n",
    "for ds in training_datasets:\n",
    "    visualize_dataset(ds)\n",
    "\n",
    "for ds in validation_datasets:\n",
    "    visualize_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17bdfc-7fe5-49d4-a98d-79ad2a47f1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a94b65-7753-4ee8-abb9-7d62f3d272e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.model_factory import model_factory\n",
    "from ml.optimizer_factory import optimizer_factory\n",
    "from ml.learner import BinarySegmentationLearner\n",
    "\n",
    "_, _, n_channels = training_datasets[0].scene.raster_source.shape\n",
    "model = model_factory(\n",
    "    config,\n",
    "    n_channels=n_channels,\n",
    ")\n",
    "\n",
    "optimizer = optimizer_factory(config, model)\n",
    "\n",
    "learner = BinarySegmentationLearner(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_ds=train_dataset_merged,  # for development and debugging, use training_datasets[0] or similar to speed up\n",
    "    valid_ds=val_dataset_merged,  # for development and debugging, use training_datasets[1] or similar to speed up\n",
    "    output_dir=expanduser(\"~/sandmining-watch/out/OUTPUT_DIR\"),\n",
    ")\n",
    "\n",
    "learner.log_data_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f30020",
   "metadata": {},
   "source": [
    "#### Check GPU Activity\n",
    "\n",
    "You can continuously monitor your GPU activity by using the command in the terminal\n",
    "\n",
    "\n",
    "`watch -d -n 0.5 nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad755047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want to log the run to W&B. You might need to authenticate to W&B.\n",
    "learner.initialize_wandb_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a710a-38ad-4d7b-9b4c-b19253b97dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learner.train(epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6846be",
   "metadata": {},
   "source": [
    "## Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize evaluation_datasets and predictor.\n",
    "evaluation_datasets and validation_datasets are based on identical scenes, but have different sliding window configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.learner import BinarySegmentationPredictor\n",
    "from utils.rastervision_pipeline import scene_to_inference_ds\n",
    "\n",
    "evaluation_datasets =  [\n",
    "    scene_to_inference_ds(\n",
    "        config, scene, full_image=True\n",
    "    ) for scene in validation_scenes\n",
    "]\n",
    "\n",
    "predictor = BinarySegmentationPredictor(\n",
    "    config,\n",
    "    model,\n",
    ")\n",
    "\n",
    "# # Alternatively: specify path to trained weights\n",
    "# path_to_weights = expanduser(\"~/sandmining-watch/out/1102-satmae-1/last-model.pth\")\n",
    "# predictor = BinarySegmentationPredictor(\n",
    "#     config,\n",
    "#     model,\n",
    "#     path_to_weights,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.eval_utils import evaluate_predicitions, make_wandb_segmentation_masks, make_wandb_predicted_probs_images\n",
    "from utils.visualizing import raster_source_to_rgb\n",
    "\n",
    "prediction_results_list = []\n",
    "\n",
    "for ds in evaluation_datasets:\n",
    "    predictions = predictor.predict_mine_probability_for_site(ds)\n",
    "\n",
    "    rgb_img = raster_source_to_rgb(ds.scene.raster_source)\n",
    "    prediction_results_list.append({\n",
    "        \"predictions\": predictions,\n",
    "        \"ground_truth\": ds.scene.label_source.get_label_arr(),\n",
    "        \"rgb_img\": rgb_img,\n",
    "        \"name\": ds.scene.id\n",
    "    })\n",
    "\n",
    "evaluation_results_dict = evaluate_predicitions(prediction_results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log results to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "assert wandb.run is not None\n",
    "\n",
    "# Add lists of W&B images to dict\n",
    "evaluation_results_dict.update({\n",
    "    'Segmenation masks': make_wandb_segmentation_masks(prediction_results_list),\n",
    "    'Predicted probabilites': make_wandb_predicted_probs_images(prediction_results_list),\n",
    "})\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log(evaluation_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rv021",
   "language": "python",
   "name": "rv021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
