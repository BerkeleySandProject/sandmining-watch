{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from os.path import expanduser\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from google.cloud import storage\n",
    "from project_config import GCP_PROJECT_NAME, DATASET_JSON_PATH\n",
    "\n",
    "gcp_client = storage.Client(project=GCP_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\" #to prevent cuda out of memory error\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#For reproducibility\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_configs.configs import lora_config, satmae_large_config_lora_methodA\n",
    "\n",
    "config = satmae_large_config_lora_methodA\n",
    "lora_config = lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "import json\n",
    "from utils.rastervision_pipeline import observation_to_scene, scene_to_training_ds, scene_to_validation_ds, scene_to_inference_ds\n",
    "from utils.data_management import observation_factory, characterize_dataset\n",
    "import random\n",
    "\n",
    "from utils.rastervision_pipeline import GoogleCloudFileSystem\n",
    "GoogleCloudFileSystem.storage_client = gcp_client\n",
    "\n",
    "#set the seed\n",
    "random.seed(13)\n",
    "\n",
    "# get the current working directory\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# define the relative path to the dataset JSON file\n",
    "json_rel_path = '../' + DATASET_JSON_PATH\n",
    "\n",
    "# combine the root directory with the relative path\n",
    "json_abs_path = os.path.join(root_dir, json_rel_path)\n",
    "\n",
    "dataset_json = json.load(open(json_abs_path, 'r'))\n",
    "\n",
    "all_scenes = [observation_to_scene(config, observation) for observation in observation_factory(dataset_json)]\n",
    "cluster_ids = [observation.cluster_id for observation in observation_factory(dataset_json)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "val_cluster_id = np.unique(cluster_ids).max() + 1\n",
    "for cid in np.unique(cluster_ids):\n",
    "    scene_idx = [i for i in range(len(cluster_ids)) if cluster_ids[i] == cid]\n",
    "    val_idx = random.sample(scene_idx, 1)[0]\n",
    "    cluster_ids[val_idx] = val_cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = [scene_to_training_ds(config, scene) for scene, cid in zip(all_scenes, cluster_ids) if cid != val_cluster_id]\n",
    "validation_datasets = [scene_to_inference_ds(config, scene, full_image=False, stride=int(config.tile_size/2)) for scene, cid in zip(all_scenes, cluster_ids) if cid == val_cluster_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_dataset_merged = ConcatDataset(training_datasets)\n",
    "val_dataset_merged = ConcatDataset(validation_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_factory import model_factory, print_trainable_parameters\n",
    "from ml.optimizer_factory import optimizer_factory\n",
    "from ml.learner_factory import learner_factory\n",
    "from experiment_configs.schemas import ThreeClassVariants\n",
    "\n",
    "_, _, n_channels = training_datasets[0].scene.raster_source.shape\n",
    "model = model_factory(\n",
    "    config,\n",
    "    n_channels=n_channels,\n",
    "    config_lora=lora_config\n",
    ")\n",
    "\n",
    "optimizer = optimizer_factory(config, model)\n",
    "\n",
    "learner = learner_factory(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_ds=train_dataset_merged,  # for development and debugging, use training_datasets[0] or similar to speed up\n",
    "    valid_ds=val_dataset_merged,  # for development and debugging, use training_datasets[1] or similar to speed up\n",
    "    output_dir=expanduser(\"~/sandmining-watch/out/OUTPUT_DIR\")\n",
    ")\n",
    "print_trainable_parameters(learner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.initialize_wandb_run()\n",
    "learner.train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rastervision_pipeline import scene_to_training_ds, scene_to_validation_ds\n",
    "from torch.utils.data import ConcatDataset\n",
    "from sklearn.model_selection import GroupKFold, LeavePGroupsOut\n",
    "import numpy as np\n",
    "\n",
    "from models.model_factory import model_factory, print_trainable_parameters\n",
    "from ml.optimizer_factory import optimizer_factory\n",
    "from ml.learner_factory import learner_factory\n",
    "from experiment_configs.schemas import ThreeClassVariants\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "class CrossValidator:\n",
    "    def __init__(self, scenes, cluster_ids, split_groups=None, num_splits=None, size_validation_group=None) -> None:\n",
    "        \"\"\"\n",
    "        split_groups is for manually assigning splits. Should be a list (number of splits in length) containing a list of \n",
    "        training and validation cluster ids. i.e. [([1, 2], [3]), ([4, 5], [6])].\n",
    "        \n",
    "        num_splits is the number of splits\n",
    "        \n",
    "        size_validation_group is used for leave p groups validation set and the rest in the training set\n",
    "        \"\"\"\n",
    "        assert (split_groups is not None) ^ (num_splits is not None) ^ (size_validation_group is not None), \"Only one of splits, num_splits, size_validation_group should not be None\"\n",
    "        \n",
    "        self.scenes = scenes\n",
    "        self.cluster_ids = np.array(cluster_ids)\n",
    "        self.splits = None\n",
    "        self.split_groups = split_groups\n",
    "        self.num_splits = num_splits\n",
    "        \n",
    "        if self.split_groups is not None:\n",
    "            self.splits = []\n",
    "            for split in self.split_groups:\n",
    "                train_split = []\n",
    "                for cid in split[0]:\n",
    "                    assert cid in self.cluster_ids, f\"Training Cluster {cid} not in the available clusters\"\n",
    "                    train_split += [i for i in range(len(self.cluster_ids)) if self.cluster_ids[i] == cid]\n",
    "                val_split = []\n",
    "                for cid in split[1]:\n",
    "                    assert cid in self.cluster_ids, f\"Validation Cluster {cid} not in the available clusters\"\n",
    "                    val_split += [i for i in range(len(self.cluster_ids)) if self.cluster_ids[i] == cid]\n",
    "                self.splits.append((np.array(train_split), np.array(val_split)))\n",
    "        elif self.num_splits is not None:\n",
    "            gkf = GroupKFold(self.num_splits)\n",
    "            scenes = list(gkf.split(np.arange(len(self.cluster_ids)), groups=self.cluster_ids))\n",
    "            self.splits = scenes\n",
    "            self.split_groups = []\n",
    "            for split in self.splits:\n",
    "                train_cids = np.unique(self.cluster_ids[split[0]])\n",
    "                val_cids = np.unique(self.cluster_ids[split[1]])\n",
    "                self.split_groups.append((train_cids, val_cids))\n",
    "        else:   # size_validation_group is not None\n",
    "            lpgo = LeavePGroupsOut(n_groups=size_validation_group)\n",
    "            scenes = list(lpgo.split(np.arange(len(self.cluster_ids)), groups=self.cluster_ids))\n",
    "            self.splits = scenes\n",
    "            self.split_groups = []\n",
    "            for split in self.splits:\n",
    "                train_cids = np.unique(self.cluster_ids[split[0]])\n",
    "                val_cids = np.unique(self.cluster_ids[split[1]])\n",
    "                self.split_groups.append((train_cids, val_cids))\n",
    "        self.num_splits = len(self.splits)\n",
    "        \n",
    "    def _train(self, model_config, lora_config, train_split, val_split, num_epochs, wandb_group_name, run_name, model_weights_output_folder):\n",
    "        train_ds = [scene_to_training_ds(model_config, self.scenes[sid]) for sid in train_split]\n",
    "        valid_ds = [scene_to_inference_ds(model_config, self.scenes[sid], full_image=False, stride=int(config.tile_size/2)) for sid in val_split]\n",
    "        train_ds_merged = ConcatDataset(train_ds)\n",
    "        valid_ds_merged = ConcatDataset(valid_ds)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        _, _, n_channels = train_ds[0].scene.raster_source.shape\n",
    "        model = model_factory(\n",
    "            model_config,\n",
    "            n_channels=n_channels,\n",
    "            config_lora=lora_config\n",
    "        )\n",
    "        \n",
    "        output_dir = expanduser(model_weights_output_folder + run_name) if model_weights_output_folder is not None else None\n",
    "        \n",
    "        optimizer = optimizer_factory(config, model)\n",
    "        learner = learner_factory(\n",
    "            config=model_config,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_ds=train_ds_merged,  # for development and debugging, use training_datasets[0] or similar to speed up\n",
    "            valid_ds=valid_ds_merged,  # for development and debugging, use training_datasets[1] or similar to speed up\n",
    "            output_dir=output_dir,\n",
    "        )\n",
    "        \n",
    "        print_trainable_parameters(learner.model)\n",
    "        # learner.initialize_wandb_run(run_name=run_name, group=wandb_group_name)\n",
    "        learner.train(epochs=num_epochs)\n",
    "        # wandb.finish()\n",
    "        del learner\n",
    "        del model\n",
    "        del train_ds\n",
    "        del valid_ds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    def cross_val(self, model_config, num_epochs, lora_config=None, wandb_group_name=None, model_weights_output_folder=None):\n",
    "        for i, (train_split, valid_split) in enumerate(self.splits):\n",
    "            self._train(model_config, \n",
    "                        lora_config, \n",
    "                        train_split, \n",
    "                        valid_split, \n",
    "                        num_epochs,\n",
    "                        wandb_group_name,\n",
    "                        \"val_cids_\" + \"_\".join([str(j) for j in self.split_groups[i][1]]),\n",
    "                        model_weights_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(all_scenes, cluster_ids, size_validation_group=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.split_groups = cv.split_groups[2:]\n",
    "cv.split_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.splits = cv.splits[2:]\n",
    "cv.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.cross_val(config,\n",
    "             1, \n",
    "             lora_config=lora_config, \n",
    "             wandb_group_name=\"SatMAE Large LoRA Method A\", \n",
    "             model_weights_output_folder=\"~/sandmining-watch/out/CrossValTest/\",\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = [scene_to_inference_ds(config, cv.scenes[sid], full_image=False, stride=int(config.tile_size/2)) for sid in cv.splits[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds_merged = ConcatDataset(valid_ds)\n",
    "val_ds_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 10\n",
    "val_dl = val_dl = DataLoader(\n",
    "            val_ds_merged,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            persistent_workers=True if num_workers > 0 else False,\n",
    "            worker_init_fn=lambda x: torch.multiprocessing.set_sharing_strategy(\"file_system\") if num_workers > 0 else None,\n",
    "            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(val_dl):\n",
    "    print(f\"{i}_x: {x.shape}\\t\\t{i}_y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rv-21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c380c98cfaa1437f7e7db6c23a3fbedb80e004445808281327d496c968591889"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
