{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf92aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6396a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import glob\n",
    "\n",
    "from rasterio.plot import show\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "from experiment_configs.configs import unet_config, satmae_large_config, satmae_large_inf_config\n",
    "\n",
    "\n",
    "from utils.rastervision_pipeline import create_s2_image_source, create_scene_s2, scene_to_inference_ds\n",
    "from ml.learner import BinarySegmentationPredictor\n",
    "from models.model_factory import model_factory\n",
    "from project_config import CLASS_CONFIG\n",
    "from ml.eval_utils import save_predictions\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a6dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 15:56:46:rastervision.pipeline.rv_config: WARNING - Root temporary directory cannot be used: /opt/data/tmp. Using root: /tmp/tmphy1udsoz\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from project_config import GCP_PROJECT_NAME, DATASET_JSON_PATH\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import gcp_utils\n",
    "\n",
    "gcp_client = storage.Client(project=GCP_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42050e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suraj.nair/.conda/envs/rio-cog-env\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ENVBIN = f\"{os.environ['HOME']}/.conda/envs/rio-cog-env\"\n",
    "print(ENVBIN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_cogs(source_file, out_file):\n",
    "    cog_create = f\"rio cogeo create {source_file} {out_file}\"\n",
    "    try:\n",
    "        subprocess.run(cog_create,\n",
    "                   cwd = f\"{ENVBIN}/bin\",\n",
    "                   shell=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd148c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(storage_client, bucket_name, out_file_name, data_for_json):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \n",
    "    json_data = json.dumps(data_for_json, indent = 2)\n",
    "    blob = bucket.blob(out_file_name)\n",
    "    blob.upload_from_string(data= json_data ,\n",
    "                            content_type='application/json')\n",
    "    \n",
    "def read_json(storage_client, bucket_name, file_name):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = BUCKET.get_blob(file_name)\n",
    "    # load blob using json\n",
    "    data = json.loads(blob.download_as_string())\n",
    "    return data\n",
    "\n",
    "\n",
    "def delete_files(filepath):\n",
    "    try:\n",
    "        cmd = f\"rm -f {filepath}/*\"\n",
    "        # Run the gsutil command\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "        print(f\"Existing file deleted\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def save_predictions_to_gcp(prediction, \n",
    "                     prediction_out_path, \n",
    "                    crs_transformer, \n",
    "                    prediction_scores_file_path, \n",
    "                    prediction_cog_file_path, \n",
    "                    gcp_dest,\n",
    "                    overwrite = False,\n",
    "                    move_to_gcp = True):\n",
    "    \n",
    "#     print(prediction_out_path, prediction_scores_file_path, prediction_cog_file_path, sep = \"\\n\")\n",
    "    ### SAVE\n",
    "    \n",
    "    print(f\"Saving to {prediction_out_path}\", datetime.now())\n",
    "    if not os.path.exists(prediction_out_path):\n",
    "        os.makedirs(prediction_out_path)\n",
    "    else:\n",
    "        if overwrite:\n",
    "            print(\"Prediction Files Exist! Overwriting..\")\n",
    "            delete_files(prediction_out_path)\n",
    "            \n",
    "        else:\n",
    "            print(\"Prediction Files Exist! Set overwrite = True to over write!\")\n",
    "            return\n",
    "        \n",
    "    save_predictions(prediction, \n",
    "         path=prediction_out_path, \n",
    "         class_config=CLASS_CONFIG, \n",
    "         crs_transformer=crs_transformer, \n",
    "         threshold=0.5)\n",
    "\n",
    "    create_cogs(prediction_scores_file_path, prediction_cog_file_path)\n",
    "\n",
    "    if move_to_gcp:\n",
    "        ### Move to GCP\n",
    "        gcp_move = f\"\"\"gsutil cp -r {prediction_cog_file_path} {gcp_dest}\"\"\"\n",
    "        try:\n",
    "            # Run the gsutil command\n",
    "            subprocess.run(gcp_move, shell=True, check=True)\n",
    "            print(f\"Files copied successfully to GCP Bucket\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error copying files: {e}\")\n",
    "\n",
    "    \n",
    "    print(\"Completed\", datetime.now())\n",
    "    \n",
    "    \n",
    "def get_river_index(dataset, river_name, date, prediction_id):\n",
    "    for i, x in enumerate(dataset):\n",
    "        if (x['date'] == DATE) & (x['river'] == river_name) & (x['uid'] == prediction_id):\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e72e7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = \"1Ov1M_zsb5jYo_dtIjUXco1wvNgasmdoZKtJ877psHL4\"\n",
    "sheet_name = \"rivers_to_osm_label\"\n",
    "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        \n",
    "df_rivers = pd.read_csv(url)\n",
    "df_rivers['osm_id'] = df_rivers['osm_id'].astype('int')\n",
    "\n",
    "river_names = df_rivers['River'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d78ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'sand_mining_inference'\n",
    "OVERWRITE = True\n",
    "N_CHANNELS = 10\n",
    "\n",
    "drop = ['Godavari', 'Godavari (N)',\n",
    "       'Godavari (S)', 'Sone - South']\n",
    "river_names = [r.lower() for r in river_names if r not in drop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "820287d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SatMae: Loading encoder weights from /data/sand_mining/checkpoints/finetuned/SatMAE-L_LoRA-bias_LN_160px_mclw-6_B8_E9_SmoothVal-S5-DecOnly-E20.pth\n",
      "Number of parameters loaded: 299\n",
      "SatMae: Loading decoder weights from /data/sand_mining/checkpoints/finetuned/SatMAE-L_LoRA-bias_LN_160px_mclw-6_B8_E9_SmoothVal-S5-DecOnly-E20.pth\n",
      "Temperature scaling set to None\n",
      "trainable params: 304.273667M || all params: 304.273667M || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "#### Configuration\n",
    "\n",
    "config = satmae_large_inf_config\n",
    "PREDICTION_ID = config.wandb_id.split(\"/\")[-1]\n",
    "\n",
    "# Load model\n",
    "from ml.model_stats import count_number_of_weights\n",
    "model = model_factory(\n",
    "    config,\n",
    "    n_channels= N_CHANNELS ,\n",
    ")\n",
    "\n",
    "predictor = BinarySegmentationPredictor(\n",
    "    config, model, config.encoder_weights_path\n",
    ")\n",
    "\n",
    "crop_sz = int(config.tile_size // 5) #20% of the tiles at the edges are discarded\n",
    "\n",
    "all_params, trainable_params = count_number_of_weights(predictor.model)\n",
    "print(f\"trainable params: {trainable_params/1e6}M || all params: {all_params/1e6}M || trainable%: {100 * trainable_params / all_params:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64479f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2022-02-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d20b4d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chambal 2022-02-01\n"
     ]
    }
   ],
   "source": [
    "### Load River URI dataset\n",
    "\n",
    "\n",
    "for river_name in ['chambal']:\n",
    "    \n",
    "    master_dataset = \"../dataset/river_oos_dataset_v0.2.json\"\n",
    "    river_jsons = json.load(open(master_dataset, 'r'))\n",
    "    \n",
    "    gc.collect()\n",
    "    if river_name not in drop:\n",
    "        print(river_name, DATE)\n",
    "        \n",
    "        BASE_PATH = f\"/data/sand_mining/predictions/outputs/{river_name}\"\n",
    "        RIVER_URI = f\"https://storage.googleapis.com/sand_mining_inference/{river_name}/{river_name}.geojson\"\n",
    "        RIVER_OUT_NAME = river_name\n",
    "        filter_func = lambda x: (x['date'] == DATE) & (x['river'] == river_name) & (x['uid'] == PREDICTION_ID)\n",
    "        \n",
    "#         river_json = list(filter(filter_func, river_jsons))[0]\n",
    "        river_index = get_river_index(river_jsons, river_name, DATE, PREDICTION_ID)\n",
    "        river_json = river_jsons.pop(river_index)\n",
    "        uri_to_s2 = river_json['uri_to_s2']\n",
    "        \n",
    "        if river_json['uri_to_prediction'] != \"\":\n",
    "            user_input = input(\"Prediction URI appears to exist! Would you like to overwrite? (Y/N)\")\n",
    "        else:\n",
    "            user_input = \"Y\"\n",
    "            \n",
    "        if user_input == \"Y\":\n",
    "            prediction_out_path = os.path.join(BASE_PATH, DATE)\n",
    "            prediction_scores_file_path = os.path.join(prediction_out_path,  \"scores.tif\")\n",
    "            prediction_cog_file_path = os.path.join(prediction_out_path, f\"{RIVER_OUT_NAME}_prediction_{DATE}.tif\")\n",
    "            gcp_dest = f\"gs://sand_mining_inference/{river_name}/{DATE}/{RIVER_OUT_NAME}_prediction_{DATE}.tif\"\n",
    "\n",
    "            prediction_base = f\"https://storage.googleapis.com/sand_mining_inference/{RIVER_OUT_NAME}\"\n",
    "            prediction_gcp_path =  f\"{prediction_base}/{DATE}/{RIVER_OUT_NAME}_prediction_{DATE}_{PREDICTION_ID}.tif\"\n",
    "            river_json['uri_to_prediction'] = prediction_gcp_path\n",
    "\n",
    "            river_jsons.append(river_json)\n",
    "            \n",
    "            with open(\"../dataset/river_oos_dataset_v0.2.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(river_jsons, f, indent=4, default=str)\n",
    "        \n",
    "        #     #Predict\n",
    "#             r_source = create_scene_s2(config, uri_to_s2, label_uri = None, scene_id = 0, rivers_uri = RIVER_URI)\n",
    "#             r_inference = scene_to_inference_ds(config, r_source, full_image=False, stride=int(config.tile_size/2))\n",
    "#             crs_transformer = r_inference.scene.raster_source.crs_transformer\n",
    "#             prediction = predictor.predict_site(r_inference, crop_sz=crop_sz)\n",
    "\n",
    "#             save_predictions_to_gcp(\n",
    "#                     prediction, \n",
    "#                     prediction_out_path, \n",
    "#                     crs_transformer, \n",
    "#                     prediction_scores_file_path, \n",
    "#                     prediction_cog_file_path, \n",
    "#                     gcp_dest,\n",
    "#                     overwrite = OVERWRITE,\n",
    "#                     move_to_gcp = True)\n",
    "\n",
    "        else:\n",
    "            print(\"Completed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rv-21",
   "language": "python",
   "name": "rv-21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
